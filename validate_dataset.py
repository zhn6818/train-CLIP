#!/usr/bin/env python3
"""
æ•°æ®é›†éªŒè¯è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
python validate_dataset.py --dataset_dir /path/to/dataset
"""

import argparse
from pathlib import Path
import random

def validate_dataset(dataset_dir):
    """éªŒè¯æ•°æ®é›†æ ¼å¼å’Œå®Œæ•´æ€§"""
    
    dataset_path = Path(dataset_dir)
    
    if not dataset_path.exists():
        print(f"âŒ é”™è¯¯: æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_path}")
        return False
    
    print(f"ğŸ” éªŒè¯æ•°æ®é›†: {dataset_path}")
    print("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ï¼ˆè®­ç»ƒ/éªŒè¯é›†ï¼‰
    subdirs = [d for d in dataset_path.iterdir() if d.is_dir()]
    
    if subdirs:
        print("ğŸ“ å‘ç°å­ç›®å½•ï¼ŒéªŒè¯æ¯ä¸ªå­ç›®å½•...")
        for subdir in subdirs:
            print(f"\néªŒè¯å­ç›®å½•: {subdir.name}")
            validate_single_directory(subdir)
    else:
        print("ğŸ“ éªŒè¯å•ä¸€ç›®å½•...")
        validate_single_directory(dataset_path)
    
    return True

def validate_single_directory(dir_path):
    """éªŒè¯å•ä¸ªç›®å½•çš„æ•°æ®é›†æ ¼å¼"""
    
    # è·å–æ‰€æœ‰æ–‡ä»¶
    image_files = list(dir_path.glob('*.jpg'))
    text_files = list(dir_path.glob('*.txt'))
    
    print(f"å›¾åƒæ–‡ä»¶: {len(image_files)} ä¸ª")
    print(f"æ–‡æœ¬æ–‡ä»¶: {len(text_files)} ä¸ª")
    
    # æ£€æŸ¥é…å¯¹
    image_stems = {f.stem for f in image_files}
    text_stems = {f.stem for f in text_files}
    paired = image_stems & text_stems
    
    print(f"æˆåŠŸé…å¯¹: {len(paired)} å¯¹")
    print(f"é…å¯¹ç‡: {len(paired) / len(image_files) * 100:.1f}%")
    
    # æ£€æŸ¥æœªé…å¯¹çš„æ–‡ä»¶
    unpaired_images = image_stems - text_stems
    unpaired_texts = text_stems - image_stems
    
    if unpaired_images:
        print(f"âš ï¸  æœªé…å¯¹çš„å›¾åƒ: {len(unpaired_images)} ä¸ª")
        if len(unpaired_images) <= 5:
            print(f"   ç¤ºä¾‹: {list(unpaired_images)[:5]}")
    
    if unpaired_texts:
        print(f"âš ï¸  æœªé…å¯¹çš„æ–‡æœ¬: {len(unpaired_texts)} ä¸ª")
        if len(unpaired_texts) <= 5:
            print(f"   ç¤ºä¾‹: {list(unpaired_texts)[:5]}")
    
    # æ£€æŸ¥æ–‡æœ¬æ–‡ä»¶å†…å®¹
    if text_files:
        print(f"\nğŸ“ æ£€æŸ¥æ–‡æœ¬æ–‡ä»¶å†…å®¹...")
        sample_files = random.sample(text_files, min(5, len(text_files)))
        
        for txt_file in sample_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    descriptions = [line.strip() for line in lines if line.strip()]
                    print(f"   {txt_file.name}: {len(descriptions)} ä¸ªæè¿°")
                    if descriptions:
                        print(f"     ç¤ºä¾‹: {descriptions[0][:50]}...")
            except Exception as e:
                print(f"   âŒ è¯»å– {txt_file.name} å¤±è´¥: {e}")
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶
    if image_files:
        print(f"\nğŸ–¼ï¸  æ£€æŸ¥å›¾åƒæ–‡ä»¶...")
        sample_files = random.sample(image_files, min(3, len(image_files)))
        
        for img_file in sample_files:
            file_size = img_file.stat().st_size
            print(f"   {img_file.name}: {file_size / 1024:.1f} KB")
    
    if len(paired) == len(image_files) and len(paired) == len(text_files):
        print("âœ… æ•°æ®é›†æ ¼å¼æ­£ç¡®!")
        return True
    else:
        print("âŒ æ•°æ®é›†æ ¼å¼æœ‰é—®é¢˜")
        return False

def main():
    parser = argparse.ArgumentParser(description='éªŒè¯CLIPè®­ç»ƒæ•°æ®é›†æ ¼å¼')
    parser.add_argument('--dataset_dir', required=True, help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    success = validate_dataset(args.dataset_dir)
    
    if success:
        print("\nğŸ‰ æ•°æ®é›†éªŒè¯å®Œæˆ!")
    else:
        print("\nğŸ’¥ æ•°æ®é›†éªŒè¯å¤±è´¥!")
        exit(1)

if __name__ == "__main__":
    main()
